{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from khaiii import KhaiiiApi\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [i for i in range(2017, 2022)]\n",
    "sortedresult = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==== 2017 ====\n",
      "========= tokenization completed =========\n",
      "# words in total :  27404\n",
      "# documents :  7275\n",
      "==== calculating tfidf ====\n",
      "   date  date    청소년     폐지     보호법     아이     소년법      교육    청소년법      학교  \\\n",
      "0  2017  2017  323.1  313.5  230.16  196.0  191.08  170.44  163.08  148.94   \n",
      "\n",
      "   ... traffic touch topic today title  tire thrill thought those pencil  \n",
      "0  ...    0.01  0.01  0.01  0.01  0.01  0.01   0.01    0.01  0.01   0.01  \n",
      "\n",
      "[1 rows x 27404 columns]\n",
      "==== completed ====\n",
      "==== 2018 ====\n",
      "========= tokenization completed =========\n",
      "# words in total :  66454\n",
      "# documents :  17015\n",
      "==== calculating tfidf ====\n",
      "   date  date      아이      학생     교사      교육    어린이집      학교     유치원      시간  \\\n",
      "0  2018  2018  451.48  394.68  384.9  375.64  355.46  352.23  343.51  263.33   \n",
      "\n",
      "   ...  관할청인  시청소년   해당함  회비횡령 경인여자대  횡령사건    인준 이사회대법원    역순   의결도  \n",
      "0  ...  0.01  0.01  0.01  0.01  0.01  0.01  0.01   0.01  0.01  0.01  \n",
      "\n",
      "[1 rows x 66454 columns]\n",
      "==== completed ====\n",
      "==== 2019 ====\n",
      "========= tokenization completed =========\n",
      "# words in total :  24986\n",
      "# documents :  3030\n",
      "==== calculating tfidf ====\n",
      "   date  date     아이     학생     학교     교육    유치원    교사   어린이집     대학  ...  \\\n",
      "0  2019  2019  86.09  72.39  70.71  64.01  63.07  52.0  50.09  49.18  ...   \n",
      "\n",
      "     vj   적용상  xxxx    임조  임의수사    앞치    악독  아줌마이  쓰레기장    불원  \n",
      "0  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  \n",
      "\n",
      "[1 rows x 24986 columns]\n",
      "==== completed ====\n",
      "==== 2020 ====\n",
      "========= tokenization completed =========\n",
      "# words in total :  15696\n",
      "# documents :  1026\n",
      "==== calculating tfidf ====\n",
      "   date     아이     개학     학생     수업     등교     학교     교사   어린이집    온라인  ...  \\\n",
      "0  2020  35.98  35.43  29.52  27.89  27.18  26.83  26.42  26.28  20.33  ...   \n",
      "\n",
      "    부교수   부적합  부총장직    브로 비리백화점  컬리너리   신설학    신공    산학    승승  \n",
      "0  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  \n",
      "\n",
      "[1 rows x 15696 columns]\n",
      "==== completed ====\n",
      "==== 2021 ====\n",
      "========= tokenization completed =========\n",
      "# words in total :  4992\n",
      "# documents :  136\n",
      "==== calculating tfidf ====\n",
      "   date    아이    교사   학대    신고    정인 아동학대    아동    교육    학교  ...  사건개요    채격  \\\n",
      "0  2021  4.23  4.07  4.0  3.79  3.79  3.5  3.44  3.33  3.23  ...  0.02  0.02   \n",
      "\n",
      "     불리   방해함    발췌  민원인간    미숙   찌꺼기    모함    가게  \n",
      "0  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  \n",
      "\n",
      "[1 rows x 4993 columns]\n",
      "==== completed ====\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"==== \"+str(year[i])+\" ====\")\n",
    "    data = pd.read_csv(\"./modi_data/data_\"+str(year[i])+\".csv\")\n",
    "    #data = data.drop(['Unnamed: 0'], axis=1)\n",
    "    \n",
    "    # dfWordList = pd.read_excel(\"./khaiii_word_cor.xlsx\")\n",
    "    # dfWordList2 = pd.read_excel(\"./khaiii_word_cor_etc.xlsx\")\n",
    "    # #print(dfWordList2)\n",
    "\n",
    "    # dfWordDel = dfWordList[dfWordList[\"수정\"] == \"삭제\"]\n",
    "    # dfWordMod = dfWordList[dfWordList[\"수정\"] != \"삭제\"]\n",
    "    # dfWordDiv = dfWordList2\n",
    "    # #print(dfWordMod)\n",
    "\n",
    "    # seriesDelete = dfWordDel[\"기존\"]\n",
    "    # stopword = []\n",
    "    # for word in seriesDelete.values:\n",
    "    #     stopword.append(word)\n",
    "    # #print(stopword)\n",
    "\n",
    "    # seriesModify = dfWordMod[\"기존\"]\n",
    "    # modiword = []\n",
    "    # for word in seriesModify.values:\n",
    "    #     modiword.append(word)\n",
    "    # #print(len(modiword))\n",
    "\n",
    "    # seriesModify2 = dfWordMod[\"수정\"]\n",
    "    # modiword2 = []\n",
    "    # for word in seriesModify2.values:\n",
    "    #     modiword2.append(word)\n",
    "    # #print(len(modiword2))\n",
    "\n",
    "    # seriesDivide = dfWordDiv[\"기존\"]\n",
    "    # divword = []\n",
    "    # for word in seriesDivide.values:\n",
    "    #     divword.append(word)\n",
    "    # #print(divword)\n",
    "    # #print(len(divword))\n",
    "\n",
    "    # seriesDivide2 = dfWordDiv[\"수정\"]\n",
    "    # divword2 = []\n",
    "    # for words in seriesDivide2.values:\n",
    "    #     divword2.append(words.split(', '))\n",
    "    # #print(divword2)\n",
    "    # #print(len(divword2))\n",
    "\n",
    "\n",
    "    api = KhaiiiApi()\n",
    "    def khaiiiTokenizer(raw, pos=['NNG', 'NNP', 'NNB', 'NP', 'NR', 'SL']):#, stopword=stopword,): # 일반명사 고유명사 의존명사 대명사 수사 외국어\n",
    "        list = []\n",
    "        skip = 0\n",
    "\n",
    "        for word in api.analyze(raw): #raw data\n",
    "            #print(word)\n",
    "            \n",
    "            for i, morph in enumerate(word.morphs):\n",
    "                #print(morph.lex)\n",
    "\n",
    "                if len(morph.lex) > 1 and morph.tag in pos: #and morph.lex not in stopword: \n",
    "                    if morph.tag == 'SL':\n",
    "                        morph.lex = morph.lex.lower()\n",
    "                    # if morph.lex in divword:\n",
    "                    #     morph.lex = divword2[divword.index(morph.lex)]\n",
    "                    #     list.extend(morph.lex)\n",
    "                    # elif morph.lex in modiword:\n",
    "                    #     morph.lex = modiword2[modiword.index(morph.lex)]\n",
    "                    #     list.append(morph.lex)\n",
    "                    # else: \n",
    "                    list.append(morph.lex)\n",
    "                    \n",
    "        return list\n",
    "\n",
    "    tokenized = data['full_data'].apply(lambda row: khaiiiTokenizer(row))\n",
    "    #print(tokenized)\n",
    "    #tokenized.to_excel(\"./finaldata/0911token_full.xls\") ##############\n",
    "    print(\"========= tokenization completed =========\")\n",
    "\n",
    "    id2word = corpora.Dictionary(tokenized)\n",
    "    corpus=[id2word.doc2bow(text) for text in tokenized]\n",
    "    print(\"# words in total : \", len(id2word))\n",
    "    print(\"# documents : \", len(corpus))\n",
    "\n",
    "    #tfidf\n",
    "    print(\"==== calculating tfidf ====\")\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "    #tfidf per doc\n",
    "    tfidflist = []\n",
    "    for doc in tfidf[corpus]:\n",
    "        inner_list = [0]*len(id2word) \n",
    "        for id, freq in doc:\n",
    "            inner_list[id] = np.around(freq, decimals=2) #put tfidf value in the place matching its index\n",
    "        tfidflist.append(inner_list)\n",
    "    #print(len(tfidflist))\n",
    "\n",
    "    tfidf_df = pd.DataFrame(tfidflist)\n",
    "    tfidf_df.columns = [id2word[i] for i in range(len(id2word))] #set columns' names as words\n",
    "    #print(tfidf_df)\n",
    "\n",
    "    total_df = pd.concat([data[[\"date\", \"id\"]], tfidf_df], axis=1)\n",
    "    #print(total_df)\n",
    "    total_df.to_csv(\"./final_data/tfidf\"+str(year[i])+\".csv\") \n",
    "\n",
    "    #sum of tfidf for each word\n",
    "    columnsum = pd.DataFrame(total_df.sum(axis=0)).T\n",
    "    columnsum = columnsum.drop(['id'], axis=1)\n",
    "    columnsum['date'] = year[i]\n",
    "    #print(columnsum)\n",
    "    columnsum.to_csv(\"./final_data/sum\"+str(year[i])+\".csv\")\n",
    "\n",
    "    #sort tfidf value in descending order\n",
    "    columnsum = columnsum.sort_values(by=0, axis=1, ascending=False)\n",
    "    print(columnsum)\n",
    "    columnsum.to_csv(\"./final_data/sorted\"+str(year[i])+\".csv\")\n",
    "\n",
    "    print(\"==== completed ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========= tokenization completed =========\n",
      "# words in total :  91609\n",
      "# documents :  28482\n",
      "==== calculating tfidf ====\n",
      "            0\n",
      "date     2021\n",
      "date     2021\n",
      "아이     756.04\n",
      "학생     635.46\n",
      "교육     619.34\n",
      "...       ...\n",
      "front    0.01\n",
      "gate     0.01\n",
      "green    0.01\n",
      "guest    0.01\n",
      "find     0.01\n",
      "\n",
      "[91609 rows x 1 columns]\n",
      "==== completed ====\n"
     ]
    }
   ],
   "source": [
    "# tfidf for full data\n",
    "\n",
    "data = pd.read_csv(\"./modi_data/full_data.csv\")\n",
    "#data = data.drop(['Unnamed: 0'], axis=1)\n",
    "    \n",
    "# dfWordList = pd.read_excel(\"./khaiii_word_cor.xlsx\")\n",
    "# dfWordList2 = pd.read_excel(\"./khaiii_word_cor_etc.xlsx\")\n",
    "# #print(dfWordList2)\n",
    "\n",
    "# dfWordDel = dfWordList[dfWordList[\"수정\"] == \"삭제\"]\n",
    "# dfWordMod = dfWordList[dfWordList[\"수정\"] != \"삭제\"]\n",
    "# dfWordDiv = dfWordList2\n",
    "# #print(dfWordMod)\n",
    "\n",
    "# seriesDelete = dfWordDel[\"기존\"]\n",
    "# stopword = []\n",
    "# for word in seriesDelete.values:\n",
    "#     stopword.append(word)\n",
    "# #print(stopword)\n",
    "\n",
    "# seriesModify = dfWordMod[\"기존\"]\n",
    "# modiword = []\n",
    "# for word in seriesModify.values:\n",
    "#     modiword.append(word)\n",
    "# #print(len(modiword))\n",
    "\n",
    "# seriesModify2 = dfWordMod[\"수정\"]\n",
    "# modiword2 = []\n",
    "# for word in seriesModify2.values:\n",
    "#     modiword2.append(word)\n",
    "# #print(len(modiword2))\n",
    "\n",
    "# seriesDivide = dfWordDiv[\"기존\"]\n",
    "# divword = []\n",
    "# for word in seriesDivide.values:\n",
    "#     divword.append(word)\n",
    "# #print(divword)\n",
    "# #print(len(divword))\n",
    "\n",
    "# seriesDivide2 = dfWordDiv[\"수정\"]\n",
    "# divword2 = []\n",
    "# for words in seriesDivide2.values:\n",
    "#     divword2.append(words.split(', '))\n",
    "# #print(divword2)\n",
    "# #print(len(divword2))\n",
    "\n",
    "\n",
    "api = KhaiiiApi()\n",
    "def khaiiiTokenizer(raw, pos=['NNG', 'NNP', 'NNB', 'NP', 'NR', 'SL']):#, stopword=stopword,): # 일반명사 고유명사 의존명사 대명사 수사 외국어\n",
    "    list = []\n",
    "    skip = 0\n",
    "\n",
    "    for word in api.analyze(raw): #raw data\n",
    "        #print(word)\n",
    "        \n",
    "        for i, morph in enumerate(word.morphs):\n",
    "            #print(morph.lex)\n",
    "\n",
    "            if len(morph.lex) > 1 and morph.tag in pos: #and morph.lex not in stopword: \n",
    "                if morph.tag == 'SL':\n",
    "                    morph.lex = morph.lex.lower()\n",
    "                # if morph.lex in divword:\n",
    "                #     morph.lex = divword2[divword.index(morph.lex)]\n",
    "                #     list.extend(morph.lex)\n",
    "                # elif morph.lex in modiword:\n",
    "                #     morph.lex = modiword2[modiword.index(morph.lex)]\n",
    "                #     list.append(morph.lex)\n",
    "                # else: \n",
    "                list.append(morph.lex)\n",
    "                \n",
    "    return list\n",
    "\n",
    "tokenized = data['full_data'].apply(lambda row: khaiiiTokenizer(row))\n",
    "#print(tokenized)\n",
    "#tokenized.to_excel(\"./finaldata/0911token_full.xls\") ##############\n",
    "print(\"========= tokenization completed =========\")\n",
    "\n",
    "id2word = corpora.Dictionary(tokenized)\n",
    "corpus=[id2word.doc2bow(text) for text in tokenized]\n",
    "print(\"# words in total : \", len(id2word))\n",
    "print(\"# documents : \", len(corpus))\n",
    "\n",
    "#tfidf\n",
    "print(\"==== calculating tfidf ====\")\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "#tfidf per doc\n",
    "tfidflist = []\n",
    "for doc in tfidf[corpus]:\n",
    "    inner_list = [0]*len(id2word) \n",
    "    for id, freq in doc:\n",
    "        inner_list[id] = np.around(freq, decimals=2) #put tfidf value in the place matching its index\n",
    "    tfidflist.append(inner_list)\n",
    "#print(len(tfidflist))\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidflist)\n",
    "tfidf_df.columns = [id2word[i] for i in range(len(id2word))] #set columns' names as words\n",
    "#print(tfidf_df)\n",
    "\n",
    "total_df = pd.concat([data[[\"date\", \"id\"]], tfidf_df], axis=1)\n",
    "#print(total_df)\n",
    "#total_df.to_csv(\"./final_data/tfidf_full.csv\") \n",
    "\n",
    "#sum of tfidf for each word\n",
    "columnsum = pd.DataFrame(total_df.sum(axis=0)).T\n",
    "columnsum = columnsum.drop(['id'], axis=1)\n",
    "columnsum['date'] = 2021\n",
    "#print(columnsum)\n",
    "#columnsum.to_csv(\"./final_data/sum_full.csv\")\n",
    "\n",
    "#sort tfidf value in descending order\n",
    "columnsum = columnsum.sort_values(by=0, axis=1, ascending=False).transpose()\n",
    "print(columnsum)\n",
    "columnsum.to_csv(\"./final_data/sorted_full.csv\")\n",
    "\n",
    "print(\"==== completed ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python382jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}