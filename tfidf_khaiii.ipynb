{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from khaiii import KhaiiiApi\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [i for i in range(2017, 2022)]\n",
    "sortedresult = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==== 2017 ====\n",
      "========= tokenization completed =========\n",
      "# words in total :  27404\n",
      "# documents :  7275\n",
      "==== calculating tfidf ====\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "This sheet is too large! Your sheet size is: 7275, 27406 Max sheet size is: 1048576, 16384",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ab34ed3095c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mtotal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m#print(total_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtotal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./final_data/tfidf\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m#sum of tfidf for each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001b[0m\n\u001b[1;32m   2184\u001b[0m             \u001b[0minf_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minf_rep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m         )\n\u001b[0;32m-> 2186\u001b[0;31m         formatter.write(\n\u001b[0m\u001b[1;32m   2187\u001b[0m             \u001b[0mexcel_writer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/formats/excel.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_rows\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rows\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    804\u001b[0m                 \u001b[0;34mf\"This sheet is too large! Your sheet size is: {num_rows}, {num_cols} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0;34mf\"Max sheet size is: {self.max_rows}, {self.max_cols}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This sheet is too large! Your sheet size is: 7275, 27406 Max sheet size is: 1048576, 16384"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"==== \"+str(year[i])+\" ====\")\n",
    "    data = pd.read_csv(\"./modi_data/data_\"+str(year[i])+\".csv\")\n",
    "    #data = data.drop(['Unnamed: 0'], axis=1)\n",
    "    \n",
    "    # dfWordList = pd.read_excel(\"./khaiii_word_cor.xlsx\")\n",
    "    # dfWordList2 = pd.read_excel(\"./khaiii_word_cor_etc.xlsx\")\n",
    "    # #print(dfWordList2)\n",
    "\n",
    "    # dfWordDel = dfWordList[dfWordList[\"수정\"] == \"삭제\"]\n",
    "    # dfWordMod = dfWordList[dfWordList[\"수정\"] != \"삭제\"]\n",
    "    # dfWordDiv = dfWordList2\n",
    "    # #print(dfWordMod)\n",
    "\n",
    "    # seriesDelete = dfWordDel[\"기존\"]\n",
    "    # stopword = []\n",
    "    # for word in seriesDelete.values:\n",
    "    #     stopword.append(word)\n",
    "    # #print(stopword)\n",
    "\n",
    "    # seriesModify = dfWordMod[\"기존\"]\n",
    "    # modiword = []\n",
    "    # for word in seriesModify.values:\n",
    "    #     modiword.append(word)\n",
    "    # #print(len(modiword))\n",
    "\n",
    "    # seriesModify2 = dfWordMod[\"수정\"]\n",
    "    # modiword2 = []\n",
    "    # for word in seriesModify2.values:\n",
    "    #     modiword2.append(word)\n",
    "    # #print(len(modiword2))\n",
    "\n",
    "    # seriesDivide = dfWordDiv[\"기존\"]\n",
    "    # divword = []\n",
    "    # for word in seriesDivide.values:\n",
    "    #     divword.append(word)\n",
    "    # #print(divword)\n",
    "    # #print(len(divword))\n",
    "\n",
    "    # seriesDivide2 = dfWordDiv[\"수정\"]\n",
    "    # divword2 = []\n",
    "    # for words in seriesDivide2.values:\n",
    "    #     divword2.append(words.split(', '))\n",
    "    # #print(divword2)\n",
    "    # #print(len(divword2))\n",
    "\n",
    "\n",
    "    api = KhaiiiApi()\n",
    "    def khaiiiTokenizer(raw, pos=['NNG', 'NNP', 'NNB', 'NP', 'NR', 'SL']):#, stopword=stopword,): # 일반명사 고유명사 의존명사 대명사 수사 외국어\n",
    "        list = []\n",
    "        skip = 0\n",
    "\n",
    "        for word in api.analyze(raw): #raw data\n",
    "            #print(word)\n",
    "            \n",
    "            for i, morph in enumerate(word.morphs):\n",
    "                #print(morph.lex)\n",
    "\n",
    "                if len(morph.lex) > 1 and morph.tag in pos: #and morph.lex not in stopword: \n",
    "                    if morph.tag == 'SL':\n",
    "                        morph.lex = morph.lex.lower()\n",
    "                    # if morph.lex in divword:\n",
    "                    #     morph.lex = divword2[divword.index(morph.lex)]\n",
    "                    #     list.extend(morph.lex)\n",
    "                    # elif morph.lex in modiword:\n",
    "                    #     morph.lex = modiword2[modiword.index(morph.lex)]\n",
    "                    #     list.append(morph.lex)\n",
    "                    # else: \n",
    "                    list.append(morph.lex)\n",
    "                    \n",
    "        return list\n",
    "\n",
    "    tokenized = data['full_data'].apply(lambda row: khaiiiTokenizer(row))\n",
    "    #print(tokenized)\n",
    "    #tokenized.to_excel(\"./finaldata/0911token_full.xls\") ##############\n",
    "    print(\"========= tokenization completed =========\")\n",
    "\n",
    "    id2word = corpora.Dictionary(tokenized)\n",
    "    corpus=[id2word.doc2bow(text) for text in tokenized]\n",
    "    print(\"# words in total : \", len(id2word))\n",
    "    print(\"# documents : \", len(corpus))\n",
    "\n",
    "    #tfidf\n",
    "    print(\"==== calculating tfidf ====\")\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "    #tfidf per doc\n",
    "    tfidflist = []\n",
    "    for doc in tfidf[corpus]:\n",
    "        inner_list = [0]*len(id2word) \n",
    "        for id, freq in doc:\n",
    "            inner_list[id] = np.around(freq, decimals=2) #put tfidf value in the place matching its index\n",
    "        tfidflist.append(inner_list)\n",
    "    #print(len(tfidflist))\n",
    "\n",
    "    tfidf_df = pd.DataFrame(tfidflist)\n",
    "    tfidf_df.columns = [id2word[i] for i in range(len(id2word))] #set columns' names as words\n",
    "    #print(tfidf_df)\n",
    "\n",
    "    total_df = pd.concat([data[[\"date\", \"id\"]], tfidf_df], axis=1)\n",
    "    #print(total_df)\n",
    "    total_df.to_csv(\"./final_data/tfidf\"+str(year[i])+\".csv\") \n",
    "\n",
    "    #sum of tfidf for each word\n",
    "    columnsum = pd.DataFrame(total_df.sum(axis=0)).T\n",
    "    columnsum = columnsum.drop(['id'], axis=1)\n",
    "    columnsum['date'] = year[i]\n",
    "    #print(columnsum)\n",
    "    columnsum.to_csv(\"./final_data/sum\"+str(year[i])+\".csv\")\n",
    "\n",
    "    #sort tfidf value in descending order\n",
    "    columnsum = columnsum.sort_values(by=0, axis=1, ascending=False)\n",
    "    print(columnsum)\n",
    "    columnsum.to_csv(\"./final_data/sorted\"+str(year[i])+\".csv\")\n",
    "\n",
    "    print(\"==== completed ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= tokenization completed =========\n",
      "# words in total :  5086\n",
      "# documents :  1149\n",
      "==== calculating tfidf ====\n",
      "   year     학습  프로그래밍     정보     로봇    컴퓨터    시스템     교육     sw     평가  ...  \\\n",
      "0  2020  38.82  36.43  36.23  33.03  31.51  28.13  26.34  25.67  25.39  ...   \n",
      "\n",
      "    양육자    소득    가계    영유    오전    오후    장르    주말    주중    영아  \n",
      "0  0.04  0.04  0.04  0.04  0.04  0.04  0.04  0.04  0.04  0.04  \n",
      "\n",
      "[1 rows x 5087 columns]\n",
      "==== completed ====\n"
     ]
    }
   ],
   "source": [
    "# tfidf for full data\n",
    "\n",
    "data = pd.read_csv(\"./modi_data/full_data.csv\")\n",
    "#data = data.drop(['Unnamed: 0'], axis=1)\n",
    "    \n",
    "# dfWordList = pd.read_excel(\"./khaiii_word_cor.xlsx\")\n",
    "# dfWordList2 = pd.read_excel(\"./khaiii_word_cor_etc.xlsx\")\n",
    "# #print(dfWordList2)\n",
    "\n",
    "# dfWordDel = dfWordList[dfWordList[\"수정\"] == \"삭제\"]\n",
    "# dfWordMod = dfWordList[dfWordList[\"수정\"] != \"삭제\"]\n",
    "# dfWordDiv = dfWordList2\n",
    "# #print(dfWordMod)\n",
    "\n",
    "# seriesDelete = dfWordDel[\"기존\"]\n",
    "# stopword = []\n",
    "# for word in seriesDelete.values:\n",
    "#     stopword.append(word)\n",
    "# #print(stopword)\n",
    "\n",
    "# seriesModify = dfWordMod[\"기존\"]\n",
    "# modiword = []\n",
    "# for word in seriesModify.values:\n",
    "#     modiword.append(word)\n",
    "# #print(len(modiword))\n",
    "\n",
    "# seriesModify2 = dfWordMod[\"수정\"]\n",
    "# modiword2 = []\n",
    "# for word in seriesModify2.values:\n",
    "#     modiword2.append(word)\n",
    "# #print(len(modiword2))\n",
    "\n",
    "# seriesDivide = dfWordDiv[\"기존\"]\n",
    "# divword = []\n",
    "# for word in seriesDivide.values:\n",
    "#     divword.append(word)\n",
    "# #print(divword)\n",
    "# #print(len(divword))\n",
    "\n",
    "# seriesDivide2 = dfWordDiv[\"수정\"]\n",
    "# divword2 = []\n",
    "# for words in seriesDivide2.values:\n",
    "#     divword2.append(words.split(', '))\n",
    "# #print(divword2)\n",
    "# #print(len(divword2))\n",
    "\n",
    "\n",
    "api = KhaiiiApi()\n",
    "def khaiiiTokenizer(raw, pos=['NNG', 'NNP', 'NNB', 'NP', 'NR', 'SL']):#, stopword=stopword,): # 일반명사 고유명사 의존명사 대명사 수사 외국어\n",
    "    list = []\n",
    "    skip = 0\n",
    "\n",
    "    for word in api.analyze(raw): #raw data\n",
    "        #print(word)\n",
    "        \n",
    "        for i, morph in enumerate(word.morphs):\n",
    "            #print(morph.lex)\n",
    "\n",
    "            if len(morph.lex) > 1 and morph.tag in pos: #and morph.lex not in stopword: \n",
    "                if morph.tag == 'SL':\n",
    "                    morph.lex = morph.lex.lower()\n",
    "                # if morph.lex in divword:\n",
    "                #     morph.lex = divword2[divword.index(morph.lex)]\n",
    "                #     list.extend(morph.lex)\n",
    "                # elif morph.lex in modiword:\n",
    "                #     morph.lex = modiword2[modiword.index(morph.lex)]\n",
    "                #     list.append(morph.lex)\n",
    "                # else: \n",
    "                list.append(morph.lex)\n",
    "                \n",
    "    return list\n",
    "\n",
    "tokenized = data['full_data'].apply(lambda row: khaiiiTokenizer(row))\n",
    "#print(tokenized)\n",
    "#tokenized.to_excel(\"./finaldata/0911token_full.xls\") ##############\n",
    "print(\"========= tokenization completed =========\")\n",
    "\n",
    "id2word = corpora.Dictionary(tokenized)\n",
    "corpus=[id2word.doc2bow(text) for text in tokenized]\n",
    "print(\"# words in total : \", len(id2word))\n",
    "print(\"# documents : \", len(corpus))\n",
    "\n",
    "#tfidf\n",
    "print(\"==== calculating tfidf ====\")\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "#tfidf per doc\n",
    "tfidflist = []\n",
    "for doc in tfidf[corpus]:\n",
    "    inner_list = [0]*len(id2word) \n",
    "    for id, freq in doc:\n",
    "        inner_list[id] = np.around(freq, decimals=2) #put tfidf value in the place matching its index\n",
    "    tfidflist.append(inner_list)\n",
    "#print(len(tfidflist))\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidflist)\n",
    "tfidf_df.columns = [id2word[i] for i in range(len(id2word))] #set columns' names as words\n",
    "#print(tfidf_df)\n",
    "\n",
    "total_df = pd.concat([data[[\"date\", \"id\"]], tfidf_df], axis=1)\n",
    "#print(total_df)\n",
    "total_df.to_csv(\"./final_data/tfidf_full.csv\") \n",
    "\n",
    "#sum of tfidf for each word\n",
    "columnsum = pd.DataFrame(total_df.sum(axis=0)).T\n",
    "columnsum = columnsum.drop(['id'], axis=1)\n",
    "columnsum['date'] = year[i]\n",
    "#print(columnsum)\n",
    "columnsum.to_csv(\"./final_data/sum_full.csv\")\n",
    "\n",
    "#sort tfidf value in descending order\n",
    "columnsum = columnsum.sort_values(by=0, axis=1, ascending=False)\n",
    "print(columnsum)\n",
    "columnsum.to_csv(\"./final_data/sorted_full.csv\")\n",
    "\n",
    "print(\"==== completed ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}