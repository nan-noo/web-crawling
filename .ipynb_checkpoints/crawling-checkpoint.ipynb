{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current page :  1\n",
      "new_paths :  ['/petitions/596365', '/petitions/596362', '/petitions/596354', '/petitions/596339', '/petitions/596328', '/petitions/596327', '/petitions/596325']\n",
      "id :  ['28916', '28915', '28914', '28913', '28912', '28911', '28910']\n",
      "titles :  ['2021년 교육. 2020년과 달라야합니다.', '어린이집 연장보육정책의 전면수정을 촉구합니다', '초등학생 매일등교!!강행 반대합니다.가정에 선택권을 주세요!!', '****** 통합학교를 취소하고 과밀학급을 해결해주세요', '어린이집  CCTV  실시간 조회를 위한 관련법 제정 요청 드립니다.', '2년 간 제게 학교폭력을 가하며 전치 2주의 부상을 입히고 ‘그때 더 팼어야 했는데’라고 말한 가해자의 처벌이 교내봉사 10시간입니다.', '교복지원금 횡포ㅡ학생과 학부모에게 교복 선택권을 주세요']\n",
      "nums 배열의 수로 확인하기  7\n",
      "dates :  ['2021-02-09', '2021-02-09', '2021-02-09', '2021-02-08', '2021-02-08', '2021-02-08', '2021-02-08']\n",
      "nums :  ['446', '1,177', '739', '2,600', '1,564', '1,018', '453']\n"
     ]
    }
   ],
   "source": [
    "# 4131 page까지임 \n",
    "\n",
    "for helper in range(1, 43):\n",
    "#for helper in range(1, 2):  ###test\n",
    "    driver = webdriver.Chrome(\"./chromedriver\")\n",
    "    \n",
    "    ids = []\n",
    "    titles = []\n",
    "    paths = []\n",
    "    contents = []\n",
    "    dates = []\n",
    "    nums = []\n",
    "    url = \"\"\n",
    "\n",
    "    i = (100 * (helper - 1)) + 1\n",
    "    #원래 => for page in range(i, 100+i):\n",
    "    for page in range(i, 100+i):\n",
    "    #for page in range(i, 1+i): ###test\n",
    "        if page == 4132: #page는 4131 까지밖에 없음.\n",
    "            break\n",
    "        url = 'https://www1.president.go.kr/petitions/?c=42&only=2&page={}&order=1'.format(page)\n",
    "        driver.get(url)\n",
    "        print(\"current page : \", page)\n",
    "\n",
    "        # driver.get(url)로 서버에게 get 요청을 보냈으니 response를 기다려야함. html을 받는데 걸리는 최소시간 -> 1.5s\n",
    "        time.sleep(1.5)\n",
    "        soup = bs(driver.page_source, 'html.parser') # 3초가 끝나면 새로운 page의 url로 접속하고 파서가 파싱한다.\n",
    "\n",
    "        # 현 page에서 petition 요소에 접근\n",
    "        lists = soup.find(\"div\", attrs={\"class\": \"ct_list1\"})\n",
    "        ul = lists.find(\"ul\", attrs={\"class\": \"petition_list\"})\n",
    "        a_lists = ul.find_all(\"a\", attrs={\"class\": \"cb relpy_w\"})  # 현재 페이지의 \"전체목록\" 내부에 있는 모든 a 태그들을 리스트로 반환.\n",
    "        id_lists = ul.find_all(\"div\", attrs={\"class\": \"bl_no\"})\n",
    "    \n",
    "        # path를 추가하기 전에 현재 paths 리스트에 몇개가 들어있는지 확인한다 => 2 page가 되면 paths 리스트의 7번 index부터 읽어야하기 때문\n",
    "        path_len = len(paths)\n",
    "        \n",
    "        for id in id_lists:\n",
    "            #print(id.text[3:].strip()) id\n",
    "            ids.append(id.text[3:].strip())\n",
    "        \n",
    "        for a in a_lists: # 현재 page 내에서 각 petition의 title과 path를 파싱하여 paths, titles 배열에 저장.\n",
    "            # print(a.attrs[\"href\"]) path 배열에 들어갈 경로 \"/petition/12345\"\n",
    "            paths.append(a.attrs[\"href\"])  # 각 청원의 path를 저장.\n",
    "            # print(a.text[3:].strip()) titles\n",
    "            titles.append(a.text[3:].strip())  # 각 청원의 title을 titles 배열에 저장.\n",
    "\n",
    "        new_paths = paths[path_len:] # page 별로 해당하는 path만 읽어야하므로 => 2 page가 되면 paths 리스트의 7번 index부터 읽어야하기 때문\n",
    "        print(\"new_paths : \", new_paths)\n",
    "        for path in new_paths: # 현재 page의 모든 petition들의 해당 url에 접속하여 content를 파싱.\n",
    "            url = \"https://www1.president.go.kr\" + path\n",
    "            driver.get(url)\n",
    "            soup = bs(driver.page_source, 'html.parser') # 각 url에 대하여 soup 객체를 새로 생성한다.\n",
    "\n",
    "            # contents 추가하기.\n",
    "            content = soup.find(\"div\", attrs={\"class\":\"View_write\"})\n",
    "            data = content.text.strip()\n",
    "            data = data.replace(\"\\n\", \"\") # \\n 문자도 제거\n",
    "            #print('본문 내용 : ', data)\n",
    "\n",
    "            # date 추가하기.\n",
    "            date_ul = soup.find(\"ul\", attrs={\"class\": \"petitionsView_info_list\"})\n",
    "            li = date_ul.li.next_sibling.next_sibling\n",
    "            li_data = li.text[4:]\n",
    "\n",
    "            # num 추가하기.\n",
    "            h2 = soup.find(\"h2\", attrs={\"class\": \"petitionsView_count\"})\n",
    "            num = h2.find('span', attrs={\"class\": \"counter\"})\n",
    "            num = num.text\n",
    "\n",
    "            contents.append(data) # 공백 제거한 data를 리스트에 저장.\n",
    "            dates.append(li_data)\n",
    "            nums.append(num)\n",
    "    driver.close()\n",
    "    # 최종 값\n",
    "    print(\"id : \", ids)\n",
    "    print(\"titles : \", titles)\n",
    "    print(\"nums 배열의 수로 확인하기 \", len(nums)) # num의 갯수가 700개(100페이지 * 7개 청원글)여야 안뺴놓고 크롤링 한거임.\n",
    "    #print(\"contents : \", contents)\n",
    "    print(\"dates : \", dates)\n",
    "    print(\"nums : \", nums)\n",
    "\n",
    "    my_dictionary = {\"id\": ids, \"title\": titles, \"date\": dates, \"num\": nums, \"contents\": contents}\n",
    "    data = pd.DataFrame(my_dictionary)  # 전체 데이터를 긁은 list인 total을 dataframe으로 변환시켜주면서 각 column의 이름을 부여해줍니다.\n",
    "\n",
    "    str_i = str(i)\n",
    "    str_i2 = str(i+99)\n",
    "    datatitle = str_i + '_' + str_i2\n",
    "    # datatitle = input(\"Please set the title of this excel file : \")  # 긁어온 데이터를 저장할 xlsx 파일의 이름을 지정해줍니다.\n",
    "    data.to_excel(\"crawling_data/\" + datatitle + \".xlsx\", engine=\"xlsxwriter\")\n",
    "    #xlsxwriter를 임포트시켜서 engine을 추가하지 않으면, data.to_excel에서 IllegalCharacterError 에러가 뜬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
